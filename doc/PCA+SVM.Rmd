---
title: "PCA + SVM"
author: "HAO HU"
date: "3/7/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,error = FALSE)
```

```{r, message=FALSE, echo=F}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}
if(!require("R.matlab")){
  install.packages("R.matlab")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("dplyr")){
  install.packages("dplyr")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("ggplot2")){
  install.packages("ggplot2")
}

if(!require("caret")){
  install.packages("caret")
}

if(!require("MASS")){
  install.packages("MASS")
}

if(!require("parallel")){
  install.packages("parallel")
}

if(!require("data.table")){
  install.packages("data.table")
}

if(!require("gbm")){
  install.packages("gbm")
}

if(!require("e1071")){
  install.packages("e1071")
}
if(!require("xgboost")){
  install.packages("xgboost")
}

if(!require("caret")){
  install.packages("caret")
}
if(!require("caTools")){
  install.packages("caTools")
}
if(!require("kernlab")){
  install.packages("kernlab")
}
if(!require("Matrix")){
  install.packages("Matrix")
}
if(!require("mlr")){
  install.packages("mlr")
}
if(!require("randomForest")){
  install.packages("randomForest")
}
if(!require("purrr")){
  install.packages("purrr")
}
if(!require("WeightedROC")){
  install.packages("WeightedROC")
}

library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
library(MASS)
library(data.table)
library(parallel)
library(gbm)
library(e1071)
library(xgboost)
library(caret)
library(caTools)
library(kernlab)
library(Matrix)
library(mlr)
library(randomForest)
library(purrr)
library(WeightedROC)
```



```{r wkdir, eval=FALSE}
set.seed(2020)
setwd("C:/Users/sluo1/Desktop/5243/other/Spring2021-Project3-group-2")
# here replace it with your own path or manually set it in RStudio to where this rmd file is located. 
# use relative path for reproducibility
```

```{r}
# svm model
hyper_grid_svm <- expand.grid(
nprinciple = c(450, 500, 550, 600, 650, 700, 750, 800)
)

```

Provide directories for training images. Training images and Training fiducial points will be in different subfolders. 
```{r}
# This will be modified for test data.
train_dir <- "../data/train_set/" 
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="") 
```

### Step 1: set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (T/F) reweighting the samples for training set 
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

#### Model selection with cross-validation
```{r exp_setup}
run.cv <- TRUE # run cross-validation on the training set
sample.reweight <- TRUE # run sample reweighting in model training
K <- 5  # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test <- TRUE # run evaluation on an independent test set
run.feature.test <- TRUE # process features for test set



run.cv.svm <- TRUE # run cross-validation on the training set for svm
run.train.svm <- TRUE # run evaluation on entire train set
run.test.svm <- TRUE # run evaluation on an independent test set
run.cv.pca <-TRUE # calculate pca


run.fudicial.list <- TRUE
sample.reweight <- TRUE # run sample reweighting in model training
```


```{r model_setup}
lmbd = c(1e-3, 5e-3, 1e-2, 5e-2, 1e-1)
model_labels = paste("LASSO Penalty with lambda =", lmbd)
```
## **Step 2: import data and train-test split**

We splitted the data to 2000 (80%) for training and 500 (20%) for test.

```{r}
#train-test split

info <- read.csv(train_label_path)
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index, train_idx)


```


```{r}
n_files <- length(list.files(train_image_dir))

#image_list <- list()
#for(i in 1:100){
#   image_list[[i]] <- readImage(paste0(train_image_dir, sprintf("%04d", i), ".jpg"))
#}

if (run.fudicial.list){
    readMat.matrix <- function(index){
        return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
}
    fiducial_pt_list <- lapply(1:n_files, readMat.matrix)
    save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
    # otherwise load the data stored for convenience
} else {
    load(file="../output/fiducial_pt_list.RData")
}
```

Fiducial points are stored in matlab format. In this step, we read them and store them in a list.
```{r read fiducial points}
#function to read fiducial points
#input: index
#output: matrix of fiducial points corresponding to the index
#readMat.matrix <- function(index, dir){
#     return(round(readMat(paste0(dir, sprintf("%04d", index), ".mat"))[[1]],0))
#}

#load fiducial points
#fiducial_pt_list <- lapply(1:n_files, readMat.matrix, dir = train_pt_dir)
#save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
```

```{r}
load("../output/fiducial_pt_list.RData")
```




### Step 3.1: feature construction

This step is converting 78 fiducial points to distances as 6006 features (3003 horizontal distances and 3003 vertical distances).

The time for training and test feature construction are as below (about 1.5s and 0.1s):

```{r}

source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
  tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
  save(dat_train, tm_feature_train, file="../output/feature_train.RData")
}else{
  load(file="../output/feature_train.RData")
}

tm_feature_test <- NA
if(run.feature.test){
  tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
  save(dat_test, tm_feature_test, file="../output/feature_test.RData")
}else{
  load(file="../output/feature_test.RData")
}

```



### Step 4.2: PCA dimension reduction

## Principal Components Analysis (PCA) + Support Vector Machines (SVMs)

### Step 4: Train a classification model with training features and responses

Call the train_svm model and test_svm model from library. 

```{r loadlib_svm, echo=FALSE}
source("../lib/train_svm.R") 
source("../lib/test_svm.R")
source("../lib/cross_validation_svm.R")
```



```{r runcv_svm}
feature_train = as.matrix(dat_train[, -6007])
label_train = as.integer(dat_train$label)
if(run.cv.pca){
  pca1 <- prcomp(feature_train)
  save(pca1,file="../output/pcaCalc.RData")
}else{
  load(file="../output/pcaCalc.RData")
}
if(run.cv.svm){
  res_cv_svm <- matrix(0, nrow = length(hyper_grid_svm$nprinciple), ncol = 6)
  for(i in 1:length(hyper_grid_svm$nprinciple)){
    cat("Number of principle component = ", hyper_grid_svm$nprinciple[i], "\n")
    res_cv_svm[i,] <- cv.function(features = feature_train, labels = label_train, K,pca1,
                              np=hyper_grid_svm$nprinciple[i], reweight = sample.reweight)
    save(res_cv_svm, file="../output/res_cv_svm.RData")
  }
}else{
  load("../output/res_cv_svm.RData")
}
```


* Choose the "best" parameter value
The best is number of pc equal to 750

```{r best_model_svm}
par_best <- 750
```

* Train the model with the entire training set using the selected model (model parameter) via cross- validation.

```{r final_train_svm}
model_labels_svm = paste("PCA principle components", hyper_grid_svm$nprinciple)
if(run.train.svm){
  weight_train <- table(label_train)
  weight_train[1] <- 10
  weight_train[2] <- 1
  
  if(sample.reweight){
    tm_train_svm <- system.time(fit_train <- train(feature_train,                                   label_train,pca1,par_best, weight_train))
  }else{
    tm_train_svm <- system.time(fit_train <- train(feature_train,                                           label_train,pca1,par_best,NULL))
  }
  save(fit_train, tm_train_svm,file="../output/fit_train_svm.RData")
}else{
  load(file="../output/fit_train_svm.RData")
}
model_labels_svm
```

### Step 5: Run test on test images

```{r test_svm}
tm_test_svm = NA
feature_test <- as.matrix(dat_test[, -6007])

if(run.test.svm){
  load(file="../output/fit_train_svm.RData")
  tm_test_svm <- system.time({ prob_pred <- test(fit_train, feature_test, pca1,par_best)})
}
```

#### Evaluation

```{r evaluation_svm}
library(WeightedROC)
label_test <- as.integer(dat_test$label)
weight_test <- rep(NA, length(label_test))
for (v in unique(label_test)){
  if (as.integer(v)==2){
    weight_test[label_test == v] = 1
  }else{
    weight_test[label_test == v] = 10
  }
}
finalguess <- as.numeric(prob_pred)
accu_svm <- sum(finalguess == label_test) / sum(label_test)
tpr.fpr <- WeightedROC(as.numeric(prob_pred), label_test, weight_test)
auc_svm <- WeightedAUC(tpr.fpr)
```


```{r result_svm, echo = FALSE}
cat("The accuracy is: ", accu_svm*100, "%.\n")
cat("The AUC is", auc_svm, ".\n")
```

#### Summarize Running Time

```{r running_time_svm, echo = FALSE}
cat("Time for training model=", tm_train_svm[1], "s \n") 
cat("Time for testing model=", tm_test_svm[1], "s \n")
```













